The most commonly used API with Voiceflow is the Dialog Manager API. It's what drives the conversations on the webchat widget and acts as the backbone of any conversion happening with a Voiceflow agent.

You send a request to it with a user input, it runs your Voiceflow agent and returns the agent's responses, and your program then takes turns giving user input and getting Voiceflow output.

You can read through it's full docs here if you're interested in learning more, after this guide.

You'll also be using a user_id to identify which conversation is going on. There are more details in the DM API guide, but in short, a user_id uniquely identifies a user's conversation, and for multiple requests to go back and forth and be in the same thread, you've got to keep the same user_id. If, on the other hand, you want to have multiple users talking at the same time, they should have different (unique, and hard to guess) user_ids. For this project, we'll just use a user's name as the user_id.

ðŸ‘‰ Create a function called interact. It'll be used to communicate with Voiceflow.

Python
JavaScript

const axios = require('axios');

const api_key = 'YOUR_API_KEY_HERE'; // it should look like this: VF.DM.XXXXXXX.XXXXXX... keep this a secret!

// user_id defines who is having the conversation, e.g. steve, john.doe@gmail.com, username_464
const interact = async (user_id, request) => {
// Function implementation will go here
};

How to send requests to Voiceflow APIs
ðŸ“˜
This article is part of a guide on getting started with Voiceflow APIs

Start from the beginning here.

More specifically, we'll be using the DM API's interact endpoint, that lets us send a request and receive a response from our Voiceflow agent. Requests being sent to the DM API consist of a user_id and a request payload.

ðŸ‘‰ Add a line to your interact function that sends a post request to the Voiceflow interact endpoint. We're just going to print the response from the API for now.

Python
JavaScript

const axios = require('axios');

const api_key = 'YOUR_API_KEY_HERE'; // it should look like this: VF.DM.XXXXXXX.XXXXXX... keep this a secret!

// user_id defines who is having the conversation, e.g. steve, john.doe@gmail.com, username_464
const interact = async (user_id, request) => {
try {
const response = await axios.post(
`https://general-runtime.voiceflow.com/state/user/${user_id}/interact`,
{ request: request },
{
headers: {
'Authorization': api_key,
'versionID': 'production',
'accept': 'application/json',
'content-type': 'application/json'
}
}
);

        console.log(response.data);
    } catch (error) {
        console.error('Error interacting with Voiceflow:', error);
    }

};
In this request, you'll notice how you include your API key as an Authorization header. This is used to identify which agent we are interacting with, and that you're authorized to. We're also including a versionID alias, that just tell us to use the version that we published above.

As we discussed above, the user_id is a unique string, but the request payload is a bit more complicated.

There are two important types of payloads you need to know about:

The launch payload is used to start or reset the conversation. It's what's sent when you click Start New Chat on the web chat widget. In most projects, it's appropriate to automatically send this request when your app starts. Its payload is simply { 'type': 'launch' }.
The text payload is used to send a text reply from the user. It's the most common way of getting input from a user. Text messages sent this way are also matched for intents if you have any set up, so it can be used to select from menus or trigger intent-specific workflows. Its payload looks like { 'type': 'text', 'payload': 'my text' }.
There are a few more ways that a user can interact with the Voiceflow agent, like clicking a button, but these two are enough to get us started. You can find more documentation about the types of requests that you can send in the interact endpoint docs here.

ðŸ“˜

How to start a conversation
ðŸ“˜
This article is part of a guide on getting started with Voiceflow APIs

Start from the beginning here.

ðŸ‘‰ At the bottom of your file, add a variable called name that will take an input from the user to set the user_id. We'll also just add a single launch request to our interact function and see what it prints.

Python
JavaScript

const axios = require('axios');
const readline = require('readline');

const api_key = 'YOUR_API_KEY_HERE'; // it should look like this: VF.DM.XXXXXXX.XXXXXX... keep this a secret!

// user_id defines who is having the conversation, e.g. steve, john.doe@gmail.com, username_464
const interact = async (user_id, request) => {
try {
const response = await axios.post(
`https://general-runtime.voiceflow.com/state/user/${user_id}/interact`,
{ request: request },
{
headers: {
'Authorization': api_key,
'versionID': 'production',
'accept': 'application/json',
'content-type': 'application/json'
}
}
);

        console.log(response.data);
    } catch (error) {
        console.error('Error interacting with Voiceflow:', error);
    }

};

const rl = readline.createInterface({
input: process.stdin,
output: process.stdout
});

rl.question('> What is your name?\n', (name) => {
interact(name, { type: 'launch' }).then(() => rl.close());
});
ðŸ‘‰ Run your Pythons script with python3 VoiceflowAPIGuide.py and watch your console!

python3 VoiceflowAPIGuide.py

> What is your name?
> Testing-Alex
> [
> {

      "time":1721073897342,
      "type":"text",
      "payload":{
         [...]
         "message":"Hi there Python!",
      }

},
{
"time":1721073897342,
"type":"text",
"payload":{
[...]
"message":"Echoing",
}
}
]
We see an array of JSON objects! Congratulations on doing your first interaction with a Voiceflow API! ðŸŽ‰ ðŸ¥³.

How to parse the output traces
ðŸ“˜
This article is part of a guide on getting started with Voiceflow APIs

Start from the beginning here.

Now, let's go into how to interpret this big array of JSON objects. Each of these objects is called a trace, which represent any output from Voiceflow. Their most important fields are the type which tells you how to interpret the second part, the payload that actually stores the content you need to use the output.

So a response from the DM API is made of an array of traces, and to parse them we have to iterate through the array of traces. You can learn more about other traces here.

To get a simple output from our agent, we'll just start off with looking for text traces, that store their content in the payload's message field: trace['payload']['message'].

ðŸ‘‰ Add a for loop iterating through all the response traces, looking for text traces and printing it's output.

Python
JavaScript

import requests

api_key = 'YOUR_API_KEY_HERE' # it should look like this: VF.DM.XXXXXXX.XXXXXX... keep this a secret!

# user_id defines who is having the conversation, e.g. steve, john.doe@gmail.com, username_464

def interact(user_id, request):
response = requests.post(
f'https://general-runtime.voiceflow.com/state/user/{user_id}/interact',
json={ 'request': request },
headers={
'Authorization': api_key,
'versionID': 'production'
},
)

    for trace in response.json():
        if trace['type'] == 'text':
            print(trace['payload']['message'])

name = input('> What is your name?\n')
interact(name, { 'type': 'launch' })
You should now get a much easier to understand output in your terminal.

python3 VoiceflowAPIGuide.py

> What is your name?
> Testing-Alex
> Hi there Python!
> Echoing
> Amazing! Now we've got text messages back from our Voiceflow agent. But the conversation's just one stepâ€¦ let's figure out how to send an answer back.

ðŸ“˜

How to send user replies
ðŸ“˜
This article is part of a guide on getting started with Voiceflow APIs

Start from the beginning here.

Let's make a basic input loop now so that we can talk back and forth with our Voiceflow agent. Inside the loop, we'll take input from the user to be sent to Voiceflow agent, and then format it into a response text payload { 'type': 'text', 'payload': nextInput }.

ðŸ‘‰ Add a while (True): loop to your script to send input to your Voiceflow agent. Inside it, get input from a user, and add a call to the interact function.

Python
JavaScript

const axios = require('axios');
const readline = require('readline');

const api_key = 'YOUR_API_KEY_HERE'; // it should look like this: VF.DM.XXXXXXX.XXXXXX... keep this a secret!

// user_id defines who is having the conversation, e.g. steve, john.doe@gmail.com, username_464
const interact = async (user_id, request) => {
try {
const response = await axios.post(
`https://general-runtime.voiceflow.com/state/user/${user_id}/interact`,
{ request: request },
{
headers: {
'Authorization': api_key,
'versionID': 'production',
'accept': 'application/json',
'content-type': 'application/json'
}
}
);

        const traces = response.data;
        for (let trace of traces) {
            if (trace.type === 'text') {
                console.log(trace.payload.message);
            }
        }
    } catch (error) {
        console.error('Error interacting with Voiceflow:', error);
    }

};

const rl = readline.createInterface({
input: process.stdin,
output: process.stdout
});

const startConversation = async () => {
rl.question('> What is your name?\n', async (name) => {
await interact(name, { type: 'launch' });

        while (true) {
            rl.question('> Say something\n', async (nextInput) => {
                await interact(name, { type: 'text', payload: nextInput });
            });
        }
    });

};

startConversation();
Now, if you run the script, you should be able to talk back and forth with your agent!

python3 VoiceflowAPIGuide.py

> What is your name?
> Testing-Alex
> Hi there Python!
> Echoing
> Say something
> test
> Echo #1: test
> Say something
> tests
> Echo #2: tests
> Say something
> this is so cool!
> Echo #3: this is so cool!
> ...
> Our simple testing flow only uses a text step and a capture step to get text input from a user, and is an infinite loop. Let's switch to a slightly more advanced agent.

ðŸ“˜
Next up: How to deal with other traces (button, end, and more)

How to deal with other traces (button, end, and more)
ðŸ“˜
This article is part of a guide on getting started with Voiceflow APIs

Start from the beginning here.

ðŸ‘‰ Import the second agent example project. Make sure to run it, publish it, and the update the API key in your Python code.

Other than just the text step, you might want to be able to use buttons, but you might find that if you run the new project, the buttons don't appear in the terminal. That's because a button is a different type of trace than we have dealt with yet.

To see the traces we haven't dealt with show up in the terminal so that we can debug, we'll add a catch-all trace behavior. We recommend to always add this kind of catch-all printing, as it's super helpful when you're developing and debugging.

ðŸ‘‰ Add an else: case to the trace handling loop.

Python
JavaScript

const axios = require('axios');
const readline = require('readline');

const api_key = 'YOUR_API_KEY_HERE'; // it should look like this: VF.DM.XXXXXXX.XXXXXX... keep this a secret!

// user_id defines who is having the conversation, e.g. steve, john.doe@gmail.com, username_464
const interact = async (user_id, request) => {
try {
const response = await axios.post(
`https://general-runtime.voiceflow.com/state/user/${user_id}/interact`,
{ request: request },
{
headers: {
'Authorization': api_key,
'versionID': 'production',
'accept': 'application/json',
'content-type': 'application/json'
}
}
);

        const traces = response.data;
        for (let trace of traces) {
            if (trace.type === 'text') {
                console.log(trace.payload.message);
            } else {
                console.log('Unhandled trace');
                console.log(trace);
            }
        }
    } catch (error) {
        console.error('Error interacting with Voiceflow:', error);
    }

};

const rl = readline.createInterface({
input: process.stdin,
output: process.stdout
});

const startConversation = async () => {
rl.question('> What is your name?\n', async (name) => {
await interact(name, { type: 'launch' });

        while (true) {
            rl.question('> Say something\n', async (nextInput) => {
                await interact(name, { type: 'text', payload: nextInput });
            });
        }
    });

};

startConversation();
Now, when we run the agent, we'll see that the unhandled button trace is printed. (You can ignore the path trace, it's just an extra trace output by Voiceflow that indicates what path you've gone down, and shouldn't be necessary for any projects).

python3 VoiceflowAPIGuide.py
[...]
Unhandled trace
{
"time":1721077675242,
"type":"choice",
"payload":{
"buttons":[
{
"name":"Hat",
"request":{
"type":"path-mu25u3epc",
[..]
}
},
{
"name":"Shirt",
"request":{
"type":"path-if27o3ev7",
[...]
}
},
{
"name":"Neither",
"request":{
"type":"path-2v28a3eqk",
[...]
To learn how to handle each different kind of trace, it's really useful to look at this trace documentation, and we'll walk through handling the choice (buttons) trace.

The important steps are

Storing all the buttons in an array
Displaying the button options in the terminal and get input from the user
Tell voiceflow which button we've clicked.
ðŸ‘‰ To store the buttons in an array, we'll create a global array at the top of the file called buttons and store each button in it by adding a new trace case for choice, and then print all the options available.

Python
JavaScript

const axios = require('axios');
const readline = require('readline');

const api_key = 'YOUR_API_KEY_HERE'; // it should look like this: VF.DM.XXXXXXX.XXXXXX... keep this a secret!

let buttons = [];

// user_id defines who is having the conversation, e.g. steve, john.doe@gmail.com, username_464
const interact = async (user_id, request) => {
try {
const response = await axios.post(
`https://general-runtime.voiceflow.com/state/user/${user_id}/interact`,
{ request: request },
{
headers: {
'Authorization': api_key,
'versionID': 'production',
'accept': 'application/json',
'content-type': 'application/json'
}
}
);

        const traces = response.data;
        for (let trace of traces) {
            if (trace.type === 'text') {
                console.log(trace.payload.message);
            } else if (trace.type === 'choice') {
                buttons = trace.payload.buttons;
                console.log('Choose one of the following:');
                for (let i = 0; i < buttons.length; i++) {
                    console.log(`${i + 1}. ${buttons[i].name}`);
                }
            } else {
                console.log('Unhandled trace');
                console.log(trace);
            }
        }
    } catch (error) {
        console.error('Error interacting with Voiceflow:', error);
    }

};

const rl = readline.createInterface({
input: process.stdin,
output: process.stdout
});

const startConversation = async () => {
rl.question('> What is your name?\n', async (name) => {
await interact(name, { type: 'launch' });

        while (true) {
            rl.question('> Say something\n', async (nextInput) => {
                await interact(name, { type: 'text', payload: nextInput });
            });
        }
    });

};

startConversation();
This will then print something like:

Would you prefer to get a test hat or a test t-shirt?
Choose one of the following:

1. Hat
2. Shirt
3. Neither
   Now, we have to get the button selection from a user. We'll add some conditional logic to check if there are button choices available, and if so to interpret the input as an index into that list of choices. If the user selects one of the choices, we'll reply to Voiceflow with the request payload that each button had attached to it. If they give another answer, we'll pass it to Voiceflow as a normal text response, and it'll either be recognized as an intent, or go down the No Match path of the button.

ðŸ‘‰ Copy the new code below that gets a selected button from the user if there are any options, and sends the associated answer back.

Python
JavaScript

const axios = require('axios');
const readline = require('readline');

const api_key = 'YOUR_API_KEY_HERE'; // it should look like this: VF.DM.XXXXXXX.XXXXXX... keep this a secret!

let buttons = [];

// user_id defines who is having the conversation, e.g. steve, john.doe@gmail.com, username_464
const interact = async (user_id, request) => {
try {
const response = await axios.post(
`https://general-runtime.voiceflow.com/state/user/${user_id}/interact`,
{ request: request },
{
headers: {
'Authorization': api_key,
'versionID': 'production',
'accept': 'application/json',
'content-type': 'application/json'
}
}
);

        const traces = response.data;
        for (let trace of traces) {
            if (trace.type === 'text') {
                console.log(trace.payload.message);
            } else if (trace.type === 'choice') {
                buttons = trace.payload.buttons;
                console.log('Choose one of the following:');
                for (let i = 0; i < buttons.length; i++) {
                    console.log(`${i + 1}. ${buttons[i].name}`);
                }
            } else {
                console.log('Unhandled trace');
                console.log(trace);
            }
        }
    } catch (error) {
        console.error('Error interacting with Voiceflow:', error);
    }

};

const rl = readline.createInterface({
input: process.stdin,
output: process.stdout
});

const startConversation = async () => {
rl.question('> What is your name?\n', async (name) => {
await interact(name, { type: 'launch' });

        while (true) {
            if (buttons.length > 0) {
                rl.question('> Choose a button number, or a reply\n', async (buttonSelection) => {
                    try {
                        const selection = parseInt(buttonSelection);
                        if (isNaN(selection) || selection < 1 || selection > buttons.length) {
                            throw new Error('Invalid selection');
                        }
                        await interact(name, buttons[selection - 1].request);
                    } catch {
                        await interact(name, { type: 'text', payload: buttonSelection });
                    }
                    buttons = [];
                });
            } else {
                rl.question('> Say something\n', async (nextInput) => {
                    await interact(name, { type: 'text', payload: nextInput });
                });
            }
        }
    });

};

startConversation();
Now, we can answer 2 to select a Shirt!

Handling buttons is similar to how many other traces are handled, so as you expand your interface using the DM API, you'll implement more and more traces.

âœ¨
Extra Challenge

The provided flow includes two images that are shared when you choose either a shirt or a hat. Try handling those traces and displaying the images from inside your Python script! As a hint, you could use matplotlib, pillow, or IPython.display to show the images. Good luck!

One last trace that's important to handle is the end trace. It's triggered by Voiceflow when there's no more steps attached to a path, or you hit the end step. You might have seen it after choosing one of the options in your handy catch-all trace print: {'type': 'end'}.

ðŸ‘‰ To handle the end trace, we'll store a boolean called isRunning that's updated each time we interact with Voiceflow, and represents if we've encountered the end trace, and we'll loop while isRunning is true, and set it false if we encounter the end trace.

Python
JavaScript

const axios = require('axios');
const readline = require('readline');

const api_key = 'YOUR_API_KEY_HERE'; // it should look like this: VF.DM.XXXXXXX.XXXXXX... keep this a secret!

let buttons = [];

// Function to interact with Voiceflow API
const interact = async (user_id, request) => {
try {
const response = await axios.post(
`https://general-runtime.voiceflow.com/state/user/${user_id}/interact`,
{ request: request },
{
headers: {
'Authorization': api_key,
'versionID': 'production',
'accept': 'application/json',
'content-type': 'application/json'
}
}
);

        const traces = response.data;
        for (let trace of traces) {
            if (trace.type === 'text') {
                console.log(trace.payload.message);
            } else if (trace.type === 'choice') {
                buttons = trace.payload.buttons;
                console.log('Choose one of the following:');
                for (let i = 0; i < buttons.length; i++) {
                    console.log(`${i + 1}. ${buttons[i].name}`);
                }
            } else if (trace.type === 'end') {
                // End of conversation
                return false;
            } else {
                console.log('Unhandled trace');
                console.log(trace);
            }
        }
        // Conversation is still running
        return true;
    } catch (error) {
        console.error('Error interacting with Voiceflow:', error);
        return false;
    }

};

// Readline interface for user input
const rl = readline.createInterface({
input: process.stdin,
output: process.stdout
});

// Function to start the conversation
const startConversation = async () => {
rl.question('> What is your name?\n', async (name) => {
let isRunning = await interact(name, { type: 'launch' });

        while (isRunning) {
            if (buttons.length > 0) {
                rl.question('> Choose a button number, or a reply\n', async (buttonSelection) => {
                    try {
                        isRunning = await interact(name, buttons[parseInt(buttonSelection) - 1].request);
                    } catch {
                        isRunning = await interact(name, { type: 'text', payload: buttonSelection });
                    }
                    buttons = [];
                });
            } else {
                rl.question('> Say something\n', async (nextInput) => {
                    isRunning = await interact(name, { type: 'text', payload: nextInput });
                });
            }
        }

        console.log('The conversation has ended.');
        rl.close();
    });

};

startConversation();
Congratulations! We've made a nice little Voiceflow conversation interface here, and you can extrapolate lots of the lessons we've learned here to build apps and custom interfaces like the EduChat Article Reader or the Unity AI NPC.

Agent API key
An Agent API key (also sometimes referred to as a Dialog Manager API key) is a key with the prefix VF.DM.. These keys are how you can access all the APIs related to your project.

Obtaining an Agent API key
To access the Agent API key for a specific project:

Open the agent you want to connect with
Select on the Integrations tab (shortcut: 3)
Copy the key.
You can also generate secondary Agent API keys for API key rotations (so you can always have one valid key up). After rotating your API keys, you should immediately promote the secondary key to primary.

ðŸ“˜
Remember, you can always revoke and re-generate a new Agent API key from this same integrations tab in case it's exposed.

Using a Version Alias
For the versionID header, you should pass in the value:

'development' for testing and development purposes
'production' for live apps and production deployments

Versions and Project IDs
You can find your Project ID and Version ID in the general settings. This article will explain what they are and why you might care about using them.

Versions
Voiceflow agents use versions to be able to choose a specific version of their agent to run with our Dialog Manager API (DM API) or the Chat Widget, and is also required for some other APIs.

The two main versions available are:

development: A new development version is compiled each time you click on the Play button

production: A new production version is published each time you click on the Publish button

A development version is meant to be used for testing purpose, when you are making changes to your agent and want to test them before pushing those changes to your live agent (production). A production version should be a stable, tested version you want to use in production while youâ€™re working on the development version of your agent in Voiceflow Creator.

Each version has a unique version ID associated, but your current production and development version can often be referred to using their alias. The production version's alias is production and the development version's alias is development. This is useful because it means you don't have to update the version ID being used in your code if you publish a new version.

Note: A couple of APIs might still next the exact version ID instead of an alias like in the versions API.

When you make a request to the DM API without passing any version in the header, we default to the development version (same goes for the Chat Widget snippet code).

Below, a DM API launch request using the development version of an agent

cURL

curl --request POST \
 --url https://general-runtime.voiceflow.com/state/user/demo/interact \
 --header 'Authorization: VF.DM.XYZ' \
 --header 'Content-Type: application/json' \
 --header 'versionID: development' \
 --data '{
"action": {
"type": "launch"
}
}'
Below, some Chat Widget snippet code loading the production version of an agent

HTML

<script type="text/javascript">
  (function(d, t) {
      var v = d.createElement(t), s = d.getElementsByTagName(t)[0];
      v.onload = function() {
        window.voiceflow.chat.load({
          verify: { projectID: 'xyz' },
          url: 'https://general-runtime.voiceflow.com',
          versionID: 'production'
        });
      }
      v.src = "https://cdn.voiceflow.com/widget/bundle.mjs"; v.type = "text/javascript"; s.parentNode.insertBefore(v, s);
  })(document, 'script');
</script>

Again, always having two different versions allow you to keep working on/updating your agent (development) without impacting an existing live version (production).

Project IDs
Project IDs are only required for certain APIs, and is also used in the web chat's code snippet. It can be found from the Settings â†’ General page, and uniquely identifies your project (and never changes).

For the DM API for example, the link to the project is done through the API key directly, but some APIs the like the Transcripts API for example, needs both the authorization and the Project ID.

In general, you'll only need to use the API key, and your applications can have a local variable storing the project ID for when it's needed, but in some cases you might need your Project ID.

Overview
The Dialog Manager API (DM API) allows any application to talk with a Voiceflow diagram using HTTP calls to the interact endpoint.

Managing your conversation state when using DM API
The DM API automatically creates and manages the conversation state. Identical requests to the DM API may produce different responses, depending on your diagram's logic and the previous request that the API received.

Note that this means the DM API is not a REST API as it does not satisfy the statelessness property. The DM API's responses depend not only on the request, but also stored state within the server. Keep this in mind while working with the DM API.

A diagram of how a conversation through the Dialog Management API works with example payloads, traces received, visual representations, and an example of how to integrate the Dialog Management API deeply into an app through a custom action.
A diagram of how a conversation through the Dialog Management API works with example payloads, traces received, visual representations, and an example of how to integrate the Dialog Management API deeply into an app through a custom action.

Tracking conversation state
All endpoints take in a userID parameter, which is used to identify the caller and assign them a unique conversation state object.

Multiple conversation sessions
Multiple conversation sessions to the same Voiceflow project can be running simultaneously. Each session is identified by the userID that you specify.

For example, customer A should communicate with /state/user/customerA/interact whereas customer B should communicate with /state/user/customerB/interact.

When customer A gives a response, such as "I would like a large pizza", it will advance their specific conversation session identified by /state/user/customerA/interact. For example, the app might then ask what toppings customer A wants.

Meanwhile, /state/user/customerB/interact's session remains unchanged, e.g, it might be waiting for customer B to give their order.

Format of userID
The format of userID is up to you. You can choose any string that fits your particular domain such as user1234 or 507f191e810c19729de860ea.

There are a few best practices to defining a userID format:

Unique - The userID should be unique to each user. Otherwise, if two users share the same userID, the Voiceflow app may leak information about user A's conversation to user B, which is a potential privacy violation.

Non-sensitive - It is not recommended to use sensitive or private information in the userID such
as emails, real names, or phone numbers.

versionID
DM API endpoints also accept a versionID header whose value is a version alias that points to a particular version of your Voiceflow project.

The currently supported aliases are:

development - The version displayed on the Voiceflow Creator's canvas

production - The version that has been published

Use the development alias whenever you are experimenting with your API, and the production version when integrating Voiceflow with your web app. Learn more about version and project IDs here.

Updating your version
To update the 'development' version exposed by the DM API, you must:

Make your changes on the canvas and NLU manager

Hit the blue Run button in the Voiceflow canvas to compile the diagram

Hit the "Train Assistant" button in the Prototype tool to train the NLU model

To update the 'production' version exposed by the DM API, you must:

Make your changes on the canvas and NLU manager

Hit the Publish button at the top-right corner of the Voiceflow canvas

Interact Stream
post
https://general-runtime.voiceflow.com/v2/project/{projectID}/user/{userID}/interact/stream
Sends a request to advance the conversation session with your Voiceflow project, recieving a stream of events back.

This endpoint initiates a streaming interaction session with a Voiceflow agent. Clients connect to this endpoint to receive server-side events (SSE) using the text/event-stream format. This allows for real time events during progression through the flow.

Streaming events can be used to drastically improve latency and provide a more natural conversation by sending information to the user as soon as it's ready, instead of waiting for the entire Voiceflow turn to be finished.

In the block above, events would be sent as goes:

The API immediately sends an event for Message 1 ("give me a moment")
Then a long API Call holds up the rest of the answer
Once the API call is finished, the API sends an event for Message 2 "got it...".
Streaming allows us to respond first with Message 1 before going into the long API Call (long-running-api-request).

From the user's perspective, the agent will respond "give me a moment...", and after the API finishes in 10 seconds, then "got it, your flight is booked for...". This helps prevent an awkward silence while the API runs in the background and prepares the user to wait for an action to be finished.

Streaming is great for breaking up long-running, blocking steps such as: AI Set/AI Response/Prompt, API, JavaScript, Function, KB Search.

The response is a stream of trace events, which each roughly corresponds with a step on the canvas, sent as the step is invoked. Visit Trace Types to learn about the different types of traces.

An Authorization header is required to validate the request. Learn more at: https://docs.voiceflow.com/reference/authentication

Your projectID is also required as part of the URL, find this in the agent settings page:

Note: this is not the same ID in the URL creator.voiceflow.com/project/.../

Example Request
cURL

curl --request POST \
 --url https://general-runtime.voiceflow.com/v2/project/$PROJECT_ID/user/$userID/interact/stream \
 --header 'Accept: text/event-stream' \
 --header 'Authorization: $VOICEFLOW_API_KEY' \
 --header 'content-type: application/json' \
 --data '
{
"action": {
"type": "launch"
}
}
Example Response
Response

event: trace
id: 1
data: {
"type": "text",
"payload": {
"message": "give me a moment...",
},
"time": 1725899197143
}

event: trace
id: 2
data: {
"type": "debug",
"payload": {
"type": "api",
"message": "API call successfully triggered"
},
"time": 1725899197146
}

event: trace
id: 3
data: {
"type": "text",
"payload": {
"message": "got it, your flight is booked for June 2nd, from London to Sydney.",
},
"time": 1725899197143
}

event: end
id: 4
You can check out an example project here using the API: https://github.com/voiceflow/streaming-wizard

For more details on advanced settings, reference dedicated documentation:

completion_events to stream LLM responses as they are being generated, instead of waiting for the entire response
Path Params
projectID
string
required
The ID of your Voiceflow project. You can find this in the settings for your agent. Note: this is not the same ID in the URL creator.voiceflow.com/project/.../

userID
string
required
A unique user ID specified by the caller.

The Dialog Manager API creates an independent conversation session for each user ID, allowing your app to talk with different users simultaneously.

Query Params
environment
string
Defaults to development
the environment of the project to run against, this was previously called versionID. aliases are supported, such as development and production.
The development environment is only updated when "Run" is clicked on the Voiceflow canvas.

development
completion_events
string
Defaults to false
[advanced] whether or not to break up LLM traces into streamed-in chunks - documentation

false
state
string
Defaults to false
[advanced] send back the new user state as an event.

false
Body Params
action
object | null
required
The user's response, e.g, user requests starting a conversation or advances the conversation by providing some textual response.

Launch Request

Text Request

Intent Request

Event Request
variables
object
The variables to update in the user's state. This object will be merged with the existing variables in the user's state.

Merge Variables object
Responses

200
A stream of events to display back to the user. The primary relevant event is trace. Other types of events include state and end

Response body
string
400
Bad Request.

The environment query parameter is not a valid tag or objectID reference.

401
Unauthorized.
HTTP request is missing a Dialog Manager API key in the Authorization header or the key is invalid.
This can also occur because of an invalid projectID URL parameter that can not be accessed.

404
Not Found.

The environment query parameter refers to an environment does not exist. For environment=production ensure you have published.

429
Too Many Requests.
Rate limit hit for given project. This limit can vary depending on cloud.

Interact
post
https://general-runtime.voiceflow.com/state/user/{userID}/interact
Sends a request to advance the conversation session with your Voiceflow project.

Requests
There are different types of requests that can be sent. To see a list of all request types, check out the documentation for the action field below.

To start a conversation, you should send a launch request. Then, to pass in your user's response, you should send a text request. If you have your own NLU matching, then you may want to directly send an intent request.

See the Request Examples on the right panel for more examples of valid request bodies.

Response Traces
After processing your request, the Dialog Manager API will then respond with an array of "traces" which are pieces of the overall response from the project:

JSON

[{
"type": "speak",
"payload": {
"type": "message",
"message": "would you like fries with that?"
}
}, {
"type": "visual",
"payload": {
"image": "https://voiceflow.com/pizza.png"
}
}]
In the example above, the Voiceflow project responded by saying "would you like fries with that?" and an image of a pizza. You can then display the chatbot's response and the image in your app.

There are many types of response traces. Each trace is produced by a particular block on your Voiceflow project. Expand the documentation for the 200 OK response below to see a list of possible response traces.

Runtime Logs
The logs query parameter can be used to enable debug logging, which includes log traces in the response.

Legacy responses
For legacy compatibility, you set the verbose query parameter to true to get a response similar to our legacy Stateless API.

JSON

// <- simplified verbose response body {
"state": {
"stack": [{
"programID": "home flow",
"nodeID": "yes no choice node"
}],
"storage": {},
"variables": {
"pizza_type": "pepperoni"
}
},
"trace": [{
"type": "speak",
"payload": {
"type": "message",
"message": "would you like fries with that?"
}
}, {
"type": "visual",
"payload": {
"image": "https://voiceflow.com/pizza.png"
}
}]
}
Path Params
userID
string
required
A unique user ID specified by the caller.

The Dialog Manager API creates an independent conversation session for each user ID, allowing your app to talk with different users simultaneously.

Query Params
verbose
boolean
Enables verbose responses similar to the legacy Stateless API.

This parameter exists for legacy compatibility reasons. New projects should always have this value set to false.

logs
Defaults to off
configure debug logs

Option 1

Option 2
Body Params
action
object | null
The user's response, e.g, user requests starting a conversation or advances the conversation by providing some textual response.

Launch Request

Text Request

Intent Request

Event Request
config
object
Optional settings to configure the response

Config object
state
object

state object
Headers
versionID
string
The version of your Voiceflow project to contact.

Use 'development' to contact the version on canvas or 'production' to contact the published version.

Responses

200
A sequential array of response "traces" to display back to the user. They can take a variety of types - common types are defined here.

Response body

Verbose Response

Trace

400
Bad Request.

Could refer to a number of possible errors:

'Voiceflow project was not published to production' - Caller is contacting the production version, but the project did not publish a production version.

'Cannot resolve project version' - Caller's provided API key is potentially malformed and could not be used to resolve a version alias.

'Cannot resolve version alias' - Caller's provided API key is potentially malformed and could not be used to resolve a version alias.

'Request is missing a versionID' - Request is missing a versionID.

401
Auth Key Required

HTTP request is missing a Dialog Manager API key in the Authorization header

404
Model not found. Ensure project is properly rendered.

Attempted to interact with the 'development' version but the Prototype has not been rendered by hitting the Run button on the Voiceflow canvas.

Update variables
patch
https://general-runtime.voiceflow.com/state/user/{userID}/variables
Updates the variables in the user's state by merging with the provided properties in the request body

Path Params
userID
string
required
A unique user ID specified by the caller.

The Dialog Manager API creates an independent conversation session for each user ID, allowing your app to talk with different users simultaneously.

Body Params
Contains the variables to update

Add Field
Headers
versionID
string
The version of your Voiceflow project to contact.

Use 'development' to contact the version on canvas or 'production' to contact the published version.

Response

200
OK

Response body
object
stack
array of objects
required
Contains all of the user's active flows

object
programID
string
required
The flow that the user has on the stack

diagramID
string
required
The ID of the diagram the flow belongs to

nodeID
string | null
The current block this flow is on

variables
object
The flow-scoped variables

Has additional fields
storage
object
Internal flow parameters used by the runtime

Has additional fields
commands
array of objects
object
type
string
push jump

event
object
Has additional fields
storage
object
required
Has additional fields
variables
object
required
Has additional fields
Updated about 1 year ago




Introduction to Events
Suggest Edits
Events are custom triggers that allow your agent to respond to specific actions or occurrences within the client application or platform. Think of events as signals from the userâ€™s interactionsâ€”the clicks, commands, or actions that indicate what the user is doing beyond just their conversational input. Whether itâ€™s a user clicking a â€œCheckoutâ€ button, navigating to a new page, or performing an in-app action, events enable your agent to initiate specific workflows dynamically.

In Voiceflow, events act as instructions sent by the client to the agent, telling it to trigger a particular flow. They make your agent more responsive and context-aware, allowing it to handle a wide range of user interactions and ensuring that the conversation remains relevant and engaging.



How Events Work in Voiceflow
Events in Voiceflow are custom triggers that you define and are initiated by the client application sending a request to the Voiceflow dialog manager. Hereâ€™s how the process works:

Event Definition: You create events in the Event CMS, specifying request name and descriptions that represent particular triggers.

Association with Workflows: You assign these events to specific flows in your agent, using the Event trigger in the Trigger step.

Client-Side Triggering: The client application sends a request to the Voiceflow Dialog Manager API, including the event name and any necessary data.

Workflow Initiation: The runtime processes the event request and initiates the corresponding flow within your agent.

This event-driven approach enhances your agentâ€™s ability to interact based on user actions within the client application, making it more intelligent and adaptable.

Why Use Events?
Expanding Interaction Footprint

Events allow your agent to respond to specific actions taken by the user within your application or platform. For example, when a user clicks a â€œHelpâ€ button, the client can trigger an event to initiate a support conversation with the agent.

Creating Contextual Experiences

By responding to events, your agent can provide contextually relevant interactions based on what the user is doing in your application.

Streamlining User Flows

Events enable your agent to assist users at critical points in their journey, providing guidance, confirmations, or additional information exactly when needed.

Examples of How Events Can Be Used
Scenario 1: User Clicks a Checkout Button

Event: InitiateCheckout
Trigger: The user clicks the â€œCheckoutâ€ button in your e-commerce app or website.
Action: The application sends an event request to the Voiceflow dialog manager with the event name InitiateCheckout.
Agent Action: The agent initiates a workflow to assist the user with the checkout process, such as confirming the items, offering shipping options, or applying discounts.
Workflow Message Example:
Agent: â€œYouâ€™re ready to check out! Would you like to review your order or proceed to payment?â€

Scenario 2: In-App Feature Usage

Event: FeatureTutorialStart
Trigger: The user accesses a new feature for the first time.
Action: The application detects this and sends an event request named FeatureTutorialStart.
Agent Action: The agent begins a flow to guide the user through a tutorial of the new feature.
Workflow Message Example:
Agent: â€œI see youâ€™re exploring our new dashboard. Would you like a quick tour to get familiar with its capabilities?â€

Scenario 3: User Sends a Command in a Messaging App

Event: ShowRecentTransactions
Trigger: In a messaging platform like Slack or Telegram, the user types a specific command, such as /recent_transactions.
Action: The messaging platform sends an event request to the agent with the event name ShowRecentTransactions.
Agent Action: The agent retrieves and presents the userâ€™s recent transactions.
Workflow Message Example:
Agent: â€œHere are your most recent transactions: [list of transactions]. Do you need help with anything else?â€

Scenario 4: User Navigates to a Specific Page

Event: PageVisitPricing
Trigger: The user navigates to the pricing page on your website.
Action: The website detects the page change and sends an event request named PageVisitPricing.
Agent Action: The agent offers assistance related to pricing, such as explaining different plans or answering FAQs.
Workflow Message Example:
Agent: â€œLooking at our pricing options? Iâ€™m here to help if you have any questions about our plans.â€
Learn more
Using Events: Learn how to use the events via API or Web chat.

Using Events
Suggest Edits
How Events Work in Voiceflow

Events in Voiceflow are custom triggers that you define and are invoked by the client application sending a request to the Voiceflow runtime. Hereâ€™s how the process works:

Event Definition: You create events in the Event CMS, specifying names and descriptions that represent particular triggers.

Association with Workflows: You assign these events to specific flows in your agent, using the Event Trigger in the Trigger step.

Client-Side Triggering: The client application sends a request to the Voiceflow runtime API or uses the Voiceflow Web Chat widget to trigger the event.

Workflow Initiation: The runtime processes the event request and initiates the corresponding flow within your agent.

This event-driven approach enhances your agentâ€™s ability to interact based on user actions within the client application, making it more intelligent and adaptable.

Getting Started with Events

To start using events in your agent, follow these steps:

1. Define Events in the Event CMS

Access the Event CMS within Voiceflow to create events that represent the triggers you need.

Create New Event: Specify a clear and descriptive name that corresponds to the user action.
Add Description: Provide details about what the event represents and when it should be triggered by the client.
2. Assign Events to Flows

In your agentâ€™s design:

Use the Trigger Step: Add a Trigger step to the flow you want to initiate with the event.
Select Event Trigger: Choose â€˜Eventâ€™ as the trigger type.
Choose the Event: Select your defined event from the list.
3. Implement Client-Side Triggering

Update your client application to send event requests to the Voiceflow runtime when specific user actions occur. This can be done via:

REST API Calls: Use Voiceflowâ€™s runtime REST API to send requests containing the event name and any relevant data.
Voiceflow Web Chat Widget: Utilize the window.voiceflow.chat.interact() method to trigger events directly from the web chat widget.
4. Test and Refine

Simulate User Actions: Perform the actions in your application that should trigger events to ensure your agent responds correctly.
Monitor Responses: Verify that the conversation flows as expected when events are triggered.
Iterate: Make adjustments based on testing to optimize the user experience.
Implementing Client-Side Triggering

A. Invoking Events via the API

To trigger an event from your client application using the Voiceflow Runtime API, send a POST request to the appropriate endpoint with the event payload.

API Request Payload Example

JSON

{
  "action": {
    "type": "event",
    "payload": {
      "event": {
        "name": "checkout"
      }
    }
  }
}
Explanation:

action.type: Set to "event" to indicate that youâ€™re triggering an event.
action.payload.event.name: The name of the event youâ€™ve defined in the Event CMS (e.g., "checkout").
API Request Example Using cURL

cURL

curl -X POST 'https://general-runtime.voiceflow.com/state/userID' \
  -H 'Content-Type: application/json' \
  -d '{
        "action": {
          "type": "event",
          "payload": {
            "event": {
              "name": "checkout"
            }
          }
        }
      }'
Steps to Implement

Detect User Action: In your application, detect the specific action (e.g., user clicks the â€œCheckoutâ€ button).

Send API Request: When the action occurs, send a POST request to the Voiceflow Runtime API with the event payload.

Handle Response: Process the response from the agent to update the UI or continue the conversation.

B. Invoking Events via Voiceflowâ€™s Native Web Chat Widget

If youâ€™re using Voiceflowâ€™s Web Chat widget, you can trigger events directly from your web application using JavaScript. The script should be placed within the tags of your HTML page.

Simplified Example: Triggering an Event on Button Click

HTML Code

Place this code within the tags of your HTML page:

HTML

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Your Page Title</title>
  <!-- Other head elements -->
</head>
<body>

  <!-- Your page content -->

  <!-- Example: A button to trigger an event -->
  <button id="myButton">Click Me</button>

  <!-- Voiceflow Web Chat Widget Script -->
  <script type="text/javascript">
    (function(d, t) {
      var v = d.createElement(t), s = d.getElementsByTagName(t)[0];
      v.onload = function() {
        // Initialize the Voiceflow Web Chat widget
        window.voiceflow.chat.load({
          verify: { projectID: 'YOUR_PROJECT_ID' },
          url: 'https://general-runtime.voiceflow.com',
          versionID: 'production'
        }).then(() => {

          // Add event listener to the button
          document.getElementById('myButton').addEventListener('click', () => {
            // Open the chat widget
            window.voiceflow.chat.open();

            // Send the 'button_clicked' event to Voiceflow
            window.voiceflow.chat.interact({
              type: 'event',
              payload: {
                event: {
                  name: 'button_clicked' // The event name defined in your Event CMS
                }
              }
            });
          });

        });
      };
      v.src = "https://cdn.voiceflow.com/widget/bundle.mjs";
      v.type = "text/javascript";
      s.parentNode.insertBefore(v, s);
    })(document, 'script');
  </script>

</body>
</html>
Explanation of the Code

HTML Button:
A button with the ID myButton is added to the HTML. This is the button the user will click to trigger the event.
Include the Voiceflow Web Chat Widget Script:
The widget script is included within the tags.
The script loads the Voiceflow Web Chat widget on your page.
Widget Initialization:
The window.voiceflow.chat.load() function is called within the v.onload callback to ensure it runs after the widget script is loaded.
The .then() method is used to run code after the widget is fully initialized.
Setting Up the Event Listener:
Within the .then() block, an event listener is added to the button with id="myButton".
When the button is clicked:
The chat widget is opened using window.voiceflow.chat.open().
An event named 'button_clicked' is sent to the agent using window.voiceflow.chat.interact().
Steps to Implement

Define the Event in Voiceflow:
Go to the Event CMS in your Voiceflow project.
Create a new event named button_clicked.
Add a description if desired.
Associate the Event with a Flow:
In your agentâ€™s canvas, add a Trigger step where you want the flow to start.
Set the trigger type to Event and select button_clicked from the list.
Add the Button to Your Web Page:
Place the HTML code for the button in your web page where appropriate.
Include the Web Chat Widget Script:
Ensure the widget script is included within the tags of your HTML.
Test the Implementation:
Open your web page in a browser.
Click the button labeled â€œClick Meâ€.
The chat widget should open, and the agent should respond according to the flow youâ€™ve designed for the button_clicked event.
Additional Notes

Consistency in Event Names: Ensure that the event name used in window.voiceflow.chat.interact() matches exactly with the event name defined in your Event CMS and associated with a Trigger step.
Example: Triggering an Event on â€œSubscribeâ€ Button Click

If you have a â€œSubscribeâ€ button in your HTML:

HTML

<button id="subscribeButton">Subscribe</button>
You can set up an event listener for it:

HTML

<!-- Voiceflow Web Chat Widget Script -->
<script type="text/javascript">
  (function(d, t) {
    var v = d.createElement(t), s = d.getElementsByTagName(t)[0];
    v.onload = function() {
      // Initialize the Voiceflow Web Chat widget
      window.voiceflow.chat.load({
        verify: { projectID: 'YOUR_PROJECT_ID' },
        url: 'https://general-runtime.voiceflow.com',
        versionID: 'production'
      }).then(() => {

        // Add event listener to the "Subscribe" button
        document.getElementById('subscribeButton').addEventListener('click', () => {
          // Open the chat widget
          window.voiceflow.chat.open();

          // Send the 'newsletter_signup' event to Voiceflow
          window.voiceflow.chat.interact({
            type: 'event',
            payload: {
              event: {
                name: 'newsletter_signup' // The event name defined in your Event CMS
              }
            }
          });
        });

      });
    };
    v.src = "https://cdn.voiceflow.com/widget/bundle.mjs";
    v.type = "text/javascript";
    s.parentNode.insertBefore(v, s);
  })(document, 'script');
</script>
Best Practices for Using Events
Be Specific with Request Names
Use clear, descriptive request names for your events to ensure they are easily identifiable and manageable.
Good Example: userClickedCheckout
Poor Example: Event1
Consider the User Experience
Ensure that the agentâ€™s responses to events are contextually appropriate and enhance the userâ€™s interaction.
Relevance: Only trigger events that provide value to the user in that context.
Timing: Trigger events at appropriate moments in the user journey to avoid overwhelming the user.
Personalization: Tailor messages to the userâ€™s actions and preferences.
